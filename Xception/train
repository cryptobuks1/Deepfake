
from data import deepfakedata
from xception import SeparableConv2d ,Block , Xception
import time
import torch
from torchvision import datasets, models, transforms
import os
import torch.nn as nn
import copy
import torch.optim as optim
import torchvision
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader ,Dataset , random_split
import cv2
import numpy as np


model = Xception()

criterion = nn.CrossEntropyLoss()

optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

################### data Area  
face = deepfakedata(root_dir='D:/framereal')      # real video 
deepfake = deepfakedata(root_dir='D:/framesynthesis') # fake video

t , v = random_split(face,[320,30] )
t1 , v1 = random_split(deepfake,[320,30] )

DATA = DataLoader( t+t1 , batch_size = 1 , shuffle = True , num_workers = 0 )
v_DATA = DataLoader( v+v1 , batch_size = 1 , shuffle = True , num_workers = 0 )

adata = {'train':DATA ,'val':v_DATA}
###################

def train_model(model , criterion , optimizer ,scheduler , num_epochs = 100):
    
   
    since = time.time()
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0 
   
    
    for epoch in range(num_epochs):
       print('Epoch : {}/{}'.format(epoch, num_epochs - 1))
       print('-' * 10)
       
       for phase in ['train','val']:
           if phase == 'train':
               model.train()
           else:
               model.eval()
        
           run_lose = 0.0
           run_correct = 0
           
           for thing in enumerate(adata[phase]):
               
               a = thing[1]['image'].to(device , dtype =torch.float32)
               
               
               b = torch.Tensor(thing[1]['label'] )
               b = b.to(device , dtype =torch.long)
               
               optimizer.zero_grad()
               
               with torch.set_grad_enabled(phase == 'train'):
                   outputs = model(a[0])
                   
                   _, preds = torch.max(outputs, 1)
                   loss = criterion(outputs , b )
                   
                # backward + optimize only if in training phase
                   if (phase == 'train'):
                       
                       loss.backward()
                       optimizer.step()

               run_lose += loss.item() * a[0].size(0)
               run_correct += torch.sum(preds == b.data)
           if phase == 'train':
                scheduler.step()
                
           epoch_loss = run_lose / (adata[phase].__len__()* a[0].size(0))
           
           epoch_acc = run_correct.double() / (adata[phase].__len__()* a[0].size(0))
           
           print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))
           
           if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                
       print()
       
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))       
           
    model.load_state_dict(best_model_wts)
    return model

       
                  
train_model(model, criterion, optimizer_ft, exp_lr_scheduler)







